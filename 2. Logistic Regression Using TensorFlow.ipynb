{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Logistic Regression Using TensorFlow\n",
    "==================================\n",
    "\n",
    "In this notebook, we will go through the steps for performing Logistic Regression on using TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Loading Libraries and Dependencies\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Part : Problem\n",
    "--------------------\n",
    "\n",
    "In this example we will use the dataset available at http://www.ats.ucla.edu/stat/data/binary.csv.\n",
    "\n",
    "This dataset has a binary response (outcome, dependent) variable called admit. There are three predictor variables: gre, gpa and rank. We will treat the variables gre and gpa as continuous. The variable rank takes on the values 1 through 4. Institutions with a rank of 1 have the highest prestige, while those with a rank of 4 have the lowest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error after 0 rounds is 0.693146586418\n",
      "The error after 10000 rounds is 0.579909801483\n",
      "The error after 20000 rounds is 0.579902887344\n",
      "The error after 30000 rounds is 0.57989603281\n",
      "The error after 40000 rounds is 0.57988935709\n",
      "The error after 50000 rounds is 0.579882442951\n",
      "The error after 60000 rounds is 0.579875767231\n",
      "The error after 70000 rounds is 0.579868912697\n",
      "The error after 80000 rounds is 0.579862058163\n",
      "The error after 90000 rounds is 0.579855203629\n",
      "The Weight term is [[ 0.18508475]\n",
      " [ 0.1481425 ]\n",
      " [-0.51791555]] and bias term is [-0.84787732]\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data = np.genfromtxt(\"http://www.ats.ucla.edu/stat/data/binary.csv\", delimiter= \",\")[1:, :]\n",
    "\n",
    "# Normalize data\n",
    "data_norm = (data - np.mean(data, axis = 0)) / np.var(data, axis = 0)\n",
    "\n",
    "# Generate x and y values\n",
    "y_values = data[:, 0]\n",
    "x_values = data_norm[:, 1:]\n",
    "\n",
    "# Create the variables required for the training\n",
    "x = tf.placeholder(\"float\", shape=[None, 3])\n",
    "y = tf.placeholder(\"float\", shape=[None, 1])\n",
    "\n",
    "# Create weight and bias terms\n",
    "W = tf.Variable(tf.zeros([3, 1]))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "# Output of prediction\n",
    "y_model = tf.sigmoid(tf.matmul(x, W) + b)\n",
    "\n",
    "# Cost function of the problem\n",
    "cost = tf.reduce_mean(-(y*tf.log(y_model) + (1-y)*tf.log(1 - y_model)))\n",
    "#cost = tf.reduce_mean(tf.square(y - y_model))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.005).minimize(cost)\n",
    "\n",
    "# Initialize all variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(100000):\n",
    "        _, err = sess.run([optimizer, cost], feed_dict={x:x_values, y:y_values.reshape(y_values.shape[0], -1)})\n",
    "        \n",
    "        if i % 10000 == 0:\n",
    "            print \"The error after {0} rounds is {1}\".format(i, err)\n",
    "    W_ = sess.run(W)\n",
    "    b_ = sess.run(b)\n",
    "    \n",
    "    print \"The Weight term is {0} and bias term is {1}\".format(W_, b_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.14790055,  0.14723506, -0.50938984]]), array([-0.83534334]))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(x_values, y_values)\n",
    "\n",
    "clf.coef_, clf.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As we can see that there is a slight differnce in the weights and bias terms of what sklearn has versus what we obtained using TensorFlow. Which is accepted due to the different ways the two algorithms converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
