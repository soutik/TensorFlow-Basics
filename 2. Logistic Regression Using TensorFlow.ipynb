{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Logistic Regression Using TensorFlow\n",
    "==================================\n",
    "\n",
    "In this notebook, we will go through the steps for performing Logistic Regression on using TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Loading Libraries and Dependencies\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Part 1: Simple Problem\n",
    "--------------------\n",
    "\n",
    "In this example we will use the dataset available at http://www.ats.ucla.edu/stat/data/binary.csv.\n",
    "\n",
    "This dataset has a binary response (outcome, dependent) variable called admit. There are three predictor variables: gre, gpa and rank. We will treat the variables gre and gpa as continuous. The variable rank takes on the values 1 through 4. Institutions with a rank of 1 have the highest prestige, while those with a rank of 4 have the lowest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error after 0 rounds is 0.693146586418\n",
      "The error after 5000 rounds is 0.578329086304\n",
      "The error after 10000 rounds is 0.576968431473\n",
      "The error after 15000 rounds is 0.575898468494\n",
      "The error after 20000 rounds is 0.575113654137\n",
      "The error after 25000 rounds is 0.574604809284\n",
      "The error after 30000 rounds is 0.574353635311\n",
      "The error after 35000 rounds is 0.574302375317\n",
      "The error after 40000 rounds is 0.574302196503\n",
      "The error after 45000 rounds is 0.574302196503\n",
      "The Weight term is [[ 30.53423119]\n",
      " [  0.11225504]\n",
      " [ -0.49830255]] and bias term is [-0.85906726]\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data = np.genfromtxt(\"http://www.ats.ucla.edu/stat/data/binary.csv\", delimiter= \",\")[1:, :]\n",
    "\n",
    "# Normalize data\n",
    "data_norm = (data - np.mean(data, axis = 0)) / np.var(data, axis = 0)\n",
    "\n",
    "# Generate x and y values\n",
    "y_values = data[:, 0]\n",
    "x_values = data_norm[:, 1:]\n",
    "\n",
    "# Create the variables required for the training\n",
    "x = tf.placeholder(\"float\", shape=[None, 3])\n",
    "y = tf.placeholder(\"float\", shape=[None, 1])\n",
    "\n",
    "# Create weight and bias terms\n",
    "W = tf.Variable(tf.zeros([3, 1]))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "# Output of prediction\n",
    "y_model = tf.sigmoid(tf.matmul(x, W) + b)\n",
    "\n",
    "# Cost function of the problem\n",
    "cost = tf.reduce_mean(-(y*tf.log(y_model) + (1-y)*tf.log(1 - y_model)))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "\n",
    "# Initialize all variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(50000):\n",
    "        _, err = sess.run([optimizer, cost], feed_dict={x:x_values, y:y_values.reshape(y_values.shape[0], -1)})\n",
    "        \n",
    "        if i % 5000 == 0:\n",
    "            print \"The error after {0} rounds is {1}\".format(i, err)\n",
    "    W_ = sess.run(W)\n",
    "    b_ = sess.run(b)\n",
    "    \n",
    "    print \"The Weight term is {0} and bias term is {1}\".format(W_, b_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.14790055,  0.14723506, -0.50938984]]), array([-0.83534334]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(x_values, y_values)\n",
    "\n",
    "clf.coef_, clf.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As we can see that there is a slight differnce in the weights and bias terms of what sklearn has versus what we obtained using TensorFlow. This is probably because of the non-normalized data and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.46520147,  3.1496063 ,  3.1496063 ,  3.1496063 , -1.46520147,\n",
       "        3.1496063 ,  3.1496063 , -1.46520147,  3.1496063 , -1.46520147,\n",
       "       -1.46520147, -1.46520147,  3.1496063 , -1.46520147,  3.1496063 ,\n",
       "       -1.46520147, -1.46520147, -1.46520147, -1.46520147,  3.1496063 ,\n",
       "       -1.46520147,  3.1496063 , -1.46520147, -1.46520147,  3.1496063 ,\n",
       "        3.1496063 ,  3.1496063 ,  3.1496063 ,  3.1496063 , -1.46520147,\n",
       "       -1.46520147, -1.46520147, -1.46520147,  3.1496063 , -1.46520147,\n",
       "       -1.46520147, -1.46520147, -1.46520147,  3.1496063 ,  3.1496063 ,\n",
       "       -1.46520147,  3.1496063 ,  3.1496063 , -1.46520147, -1.46520147,\n",
       "        3.1496063 ,  3.1496063 , -1.46520147, -1.46520147, -1.46520147,\n",
       "       -1.46520147, -1.46520147, -1.46520147,  3.1496063 , -1.46520147,\n",
       "        3.1496063 , -1.46520147, -1.46520147, -1.46520147, -1.46520147,\n",
       "        3.1496063 , -1.46520147, -1.46520147,  3.1496063 , -1.46520147,\n",
       "       -1.46520147, -1.46520147, -1.46520147, -1.46520147, -1.46520147,\n",
       "       -1.46520147, -1.46520147, -1.46520147, -1.46520147, -1.46520147,\n",
       "       -1.46520147, -1.46520147,  3.1496063 , -1.46520147,  3.1496063 ,\n",
       "       -1.46520147, -1.46520147, -1.46520147, -1.46520147,  3.1496063 ,\n",
       "       -1.46520147, -1.46520147, -1.46520147, -1.46520147,  3.1496063 ,\n",
       "       -1.46520147,  3.1496063 , -1.46520147, -1.46520147,  3.1496063 ,\n",
       "       -1.46520147, -1.46520147, -1.46520147, -1.46520147, -1.46520147,\n",
       "       -1.46520147, -1.46520147, -1.46520147, -1.46520147,  3.1496063 ,\n",
       "        3.1496063 ,  3.1496063 , -1.46520147, -1.46520147, -1.46520147,\n",
       "       -1.46520147, -1.46520147, -1.46520147, -1.46520147, -1.46520147,\n",
       "       -1.46520147,  3.1496063 , -1.46520147,  3.1496063 , -1.46520147,\n",
       "        3.1496063 ,  3.1496063 , -1.46520147, -1.46520147, -1.46520147,\n",
       "       -1.46520147,  3.1496063 , -1.46520147, -1.46520147, -1.46520147,\n",
       "        3.1496063 , -1.46520147, -1.46520147, -1.46520147, -1.46520147,\n",
       "       -1.46520147, -1.46520147, -1.46520147, -1.46520147,  3.1496063 ,\n",
       "       -1.46520147,  3.1496063 , -1.46520147, -1.46520147, -1.46520147,\n",
       "       -1.46520147, -1.46520147, -1.46520147,  3.1496063 , -1.46520147,\n",
       "        3.1496063 , -1.46520147,  3.1496063 , -1.46520147, -1.46520147,\n",
       "        3.1496063 , -1.46520147,  3.1496063 , -1.46520147, -1.46520147,\n",
       "       -1.46520147, -1.46520147,  3.1496063 , -1.46520147, -1.46520147,\n",
       "       -1.46520147, -1.46520147, -1.46520147, -1.46520147, -1.46520147,\n",
       "       -1.46520147, -1.46520147, -1.46520147,  3.1496063 , -1.46520147,\n",
       "        3.1496063 , -1.46520147,  3.1496063 , -1.46520147, -1.46520147,\n",
       "       -1.46520147, -1.46520147, -1.46520147,  3.1496063 , -1.46520147,\n",
       "       -1.46520147, -1.46520147, -1.46520147, -1.46520147, -1.46520147,\n",
       "        3.1496063 , -1.46520147, -1.46520147, -1.46520147,  3.1496063 ,\n",
       "       -1.46520147, -1.46520147,  3.1496063 , -1.46520147, -1.46520147,\n",
       "       -1.46520147,  3.1496063 ,  3.1496063 , -1.46520147,  3.1496063 ,\n",
       "        3.1496063 , -1.46520147,  3.1496063 , -1.46520147, -1.46520147,\n",
       "       -1.46520147, -1.46520147, -1.46520147, -1.46520147,  3.1496063 ,\n",
       "        3.1496063 , -1.46520147,  3.1496063 , -1.46520147,  3.1496063 ,\n",
       "       -1.46520147, -1.46520147,  3.1496063 , -1.46520147, -1.46520147,\n",
       "        3.1496063 , -1.46520147, -1.46520147, -1.46520147,  3.1496063 ,\n",
       "       -1.46520147, -1.46520147, -1.46520147, -1.46520147,  3.1496063 ,\n",
       "       -1.46520147,  3.1496063 , -1.46520147, -1.46520147, -1.46520147,\n",
       "       -1.46520147,  3.1496063 ,  3.1496063 , -1.46520147, -1.46520147,\n",
       "       -1.46520147, -1.46520147, -1.46520147, -1.46520147, -1.46520147,\n",
       "       -1.46520147, -1.46520147,  3.1496063 ,  3.1496063 ,  3.1496063 ,\n",
       "       -1.46520147,  3.1496063 ,  3.1496063 , -1.46520147, -1.46520147,\n",
       "       -1.46520147, -1.46520147,  3.1496063 ,  3.1496063 ,  3.1496063 ,\n",
       "       -1.46520147, -1.46520147,  3.1496063 ,  3.1496063 , -1.46520147,\n",
       "        3.1496063 , -1.46520147,  3.1496063 , -1.46520147, -1.46520147,\n",
       "        3.1496063 , -1.46520147,  3.1496063 ,  3.1496063 ,  3.1496063 ,\n",
       "       -1.46520147, -1.46520147, -1.46520147, -1.46520147,  3.1496063 ,\n",
       "       -1.46520147,  3.1496063 ,  3.1496063 , -1.46520147, -1.46520147,\n",
       "        3.1496063 , -1.46520147, -1.46520147, -1.46520147, -1.46520147,\n",
       "       -1.46520147, -1.46520147, -1.46520147, -1.46520147, -1.46520147,\n",
       "       -1.46520147,  3.1496063 ,  3.1496063 ,  3.1496063 , -1.46520147,\n",
       "       -1.46520147,  3.1496063 , -1.46520147, -1.46520147, -1.46520147,\n",
       "       -1.46520147, -1.46520147, -1.46520147,  3.1496063 , -1.46520147,\n",
       "        3.1496063 ,  3.1496063 ,  3.1496063 ,  3.1496063 , -1.46520147,\n",
       "       -1.46520147, -1.46520147, -1.46520147, -1.46520147, -1.46520147,\n",
       "       -1.46520147, -1.46520147,  3.1496063 , -1.46520147, -1.46520147,\n",
       "       -1.46520147, -1.46520147, -1.46520147, -1.46520147,  3.1496063 ,\n",
       "        3.1496063 , -1.46520147, -1.46520147, -1.46520147,  3.1496063 ,\n",
       "       -1.46520147,  3.1496063 , -1.46520147, -1.46520147, -1.46520147,\n",
       "       -1.46520147, -1.46520147, -1.46520147, -1.46520147, -1.46520147,\n",
       "        3.1496063 , -1.46520147,  3.1496063 , -1.46520147,  3.1496063 ,\n",
       "        3.1496063 , -1.46520147, -1.46520147,  3.1496063 , -1.46520147,\n",
       "        3.1496063 ,  3.1496063 , -1.46520147, -1.46520147,  3.1496063 ,\n",
       "       -1.46520147, -1.46520147, -1.46520147, -1.46520147, -1.46520147,\n",
       "        3.1496063 ,  3.1496063 ,  3.1496063 ,  3.1496063 , -1.46520147,\n",
       "       -1.46520147, -1.46520147,  3.1496063 , -1.46520147, -1.46520147,\n",
       "       -1.46520147,  3.1496063 , -1.46520147, -1.46520147,  3.1496063 ,\n",
       "       -1.46520147,  3.1496063 , -1.46520147, -1.46520147, -1.46520147,\n",
       "        3.1496063 ,  3.1496063 ,  3.1496063 ,  3.1496063 ,  3.1496063 ,\n",
       "       -1.46520147, -1.46520147, -1.46520147, -1.46520147, -1.46520147])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
